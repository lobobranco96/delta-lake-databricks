{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9119a6b1-da4e-4b67-afdc-5c1e412e0263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from settings import create_spark_session\n",
    "from pyspark.sql.functions import col, sum as _sum, max as _max, lit\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bdcc7d-6f59-4ee4-925d-bbc03f11b896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_table(df, table_name, gold_schema, partition_col):\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(partition_col) \\\n",
    "        .saveAsTable(f\"{gold_schema}.{table_name}\")\n",
    "\n",
    "def build_fact_sales(logger, spark, silver_schema, gold_schema, ingest_date):\n",
    "    try:\n",
    "        logger.info(\"Lendo dados da Silver: orders_cleaned\")\n",
    "        orders = spark.table(f\"{silver_schema}.orders_cleaned\") \\\n",
    "            .filter(col(\"ingest_date\") == ingest_date)\n",
    "        \n",
    "        logger.info(\"Lendo dados da Silver: order_items_cleaned\")\n",
    "        order_items = spark.table(f\"{silver_schema}.order_items_cleaned\") \\\n",
    "            .filter(col(\"ingest_date\") == ingest_date)\n",
    "\n",
    "\n",
    "        logger.info(\"Lendo dados da Silver: inventory_cleaned\")\n",
    "        inventory = spark.table(f\"{silver_schema}.inventory_cleaned\")   \n",
    "\n",
    "        logger.info(\"Construindo tabela de fato: fact_sales\")\n",
    "        fact_sales = orders.join(order_items, on=\"order_id\", how=\"inner\") \\\n",
    "            .select(\n",
    "                \"order_id\", \"customer_id\", \"product_id\", \"order_date\",\n",
    "                \"quantity\", \"unit_price\", \"total_price\", \"status\"\n",
    "            ) \\\n",
    "            .withColumn(\"ingest_date\", lit(ingest_date))\n",
    "\n",
    "        logger.info(\"Escrevendo fact_sales na Gold com particionamento por ingest_date\")\n",
    "        save_table(fact_sales, \"fact_sales\", gold_schema, \"ingest_date\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao construir fact_sales: {e}\")\n",
    "\n",
    "def build_dim_customers(logger, spark, silver_schema, gold_schema, ingest_date):\n",
    "    try:\n",
    "        logger.info(\"Lendo dados da Silver: customers_cleaned\")\n",
    "        customers = spark.table(f\"{silver_schema}.customers_cleaned\")\n",
    "\n",
    "        logger.info(\"Construindo dimensão: dim_customers\")\n",
    "        dim_customers = customers.select(\n",
    "            \"customer_id\", \"name\", \"email\", \"address\", \"created_at\"\n",
    "        ).dropDuplicates([\"customer_id\"]) \\\n",
    "         .withColumn(\"ingest_date\", lit(ingest_date))\n",
    "\n",
    "        logger.info(\"Escrevendo dim_customers na Gold com particionamento por ingest_date\")\n",
    "        save_table(dim_customers, \"dim_customers\", gold_schema, \"ingest_date\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao construir dim_customers: {e}\")\n",
    "\n",
    "def build_dim_products(logger, spark, silver_schema, gold_schema, ingest_date):\n",
    "    try:\n",
    "        logger.info(\"Lendo dados da Silver: products_cleaned\")\n",
    "        products = spark.table(f\"{silver_schema}.products_cleaned\")\n",
    "\n",
    "        logger.info(\"Construindo dimensão: dim_products\")\n",
    "        dim_products = products.select(\n",
    "            \"product_id\", \"name\", \"category\", \"price\"\n",
    "        ).dropDuplicates([\"product_id\"]) \\\n",
    "         .withColumn(\"ingest_date\", lit(ingest_date))\n",
    "\n",
    "        logger.info(\"Escrevendo dim_products na Gold com particionamento por ingest_date\")\n",
    "        save_table(dim_products, \"dim_products\", gold_schema, \"ingest_date\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao construir dim_products: {e}\")\n",
    "\n",
    "def build_current_inventory(logger, spark, silver_schema, gold_schema, ingest_date):\n",
    "    try:\n",
    "        logger.info(\"Lendo dados da Silver: inventory_cleaned\")\n",
    "        inventory = spark.table(f\"{silver_schema}.inventory_cleaned\")\n",
    "\n",
    "        logger.info(\"Construindo tabela agregada: current_inventory\")\n",
    "        current_inventory = inventory.groupBy(\"product_id\") \\\n",
    "            .agg(\n",
    "                _sum(\"change\").alias(\"stock_quantity\"),\n",
    "                _max(\"timestamp\").alias(\"last_updated\")\n",
    "            ) \\\n",
    "            .withColumn(\"ingest_date\", lit(ingest_date))\n",
    "\n",
    "        logger.info(\"Escrevendo current_inventory na Gold com particionamento por ingest_date\")\n",
    "        save_table(current_inventory, \"current_inventory\", gold_schema, \"ingest_date\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao construir current_inventory: {e}\")\n",
    "\n",
    "def main(spark, silver_path, gold_schema, ingest_date):\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger = logging.getLogger(\"GoldLayer\")\n",
    "\n",
    "    # Cria o schema Gold se não existir\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {gold_schema}\")\n",
    "\n",
    "    logger.info(\"Iniciando processamento da camada Gold\")\n",
    "\n",
    "    build_fact_sales(logger, spark, silver_schema, gold_schema, ingest_date)\n",
    "    build_dim_customers(logger, spark, silver_schema, gold_schema, ingest_date)\n",
    "    build_dim_products(logger, spark, silver_schema, gold_schema, ingest_date)\n",
    "    build_current_inventory(logger, spark, silver_schema, gold_schema, ingest_date)\n",
    "\n",
    "    logger.info(\"Camada Gold finalizada com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478a6931-5624-40b7-9719-a9c90458cc8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_config = {\n",
    "    \"spark.sql.shuffle.partitions\": \"300\",\n",
    "    \"spark.default.parallelism\": \"300\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \"spark.sql.adaptive.skewJoin.enabled\": \"true\",\n",
    "    \"spark.databricks.delta.optimizeWrite.enabled\": \"true\",\n",
    "    \"spark.databricks.delta.autoCompact.enabled\": \"true\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\": \"104857600\",\n",
    "    \"spark.sql.parquet.filterPushdown\": \"true\"\n",
    "}\n",
    "\n",
    "app_name = \"GoldLayer\"\n",
    "spark = create_spark_session(app_name, gold_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "249555e4-5b9b-407c-b22d-da5cb0b35c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Explicação das configs para a camada Gold\n",
    "  - **spark.sql.shuffle.partitions = 300**\n",
    "    - A camada Gold geralmente tem dados agregados, otimizados e prontos para consumo.\n",
    "    - É comum que aqui as operações de shuffle (como joins, agregações) sejam maiores, então aumentar o número de partições para 300 ajuda a distribuir melhor a carga, evitando partições muito grandes que causam gargalos.\n",
    "\n",
    "  - **spark.default.parallelism = 300**\n",
    "    - Controla o paralelismo padrão para operações RDD e shuffle.\n",
    "    - Mantendo esse valor alto ajuda a aproveitar melhor os recursos do cluster, principalmente em operações batch de alta escala.\n",
    "\n",
    "  - **spark.sql.adaptive.enabled = true**\n",
    "    - Ativa a execução adaptativa de queries (Adaptive Query Execution - AQE).\n",
    "\n",
    "    - O Spark pode otimizar dinamicamente planos de execução durante o runtime, ajustando a quantidade de shuffle, reduzindo skew e melhorando desempenho.\n",
    "\n",
    "  - **spark.sql.adaptive.skewJoin.enabled = true**\n",
    "    - Específico para mitigar data skew em joins.\n",
    "    - Na camada Gold, onde joins entre tabelas dimensionais e fatos são comuns, isso ajuda a evitar que partições muito desbalanceadas causem lentidão.\n",
    "\n",
    "  - **spark.databricks.delta.optimizeWrite.enabled = true**\n",
    "    - Otimiza a escrita dos arquivos Delta, criando arquivos maiores e menos fragmentados.\n",
    "    - Importante para a camada Gold, para melhorar a performance de leitura e compactação.\n",
    "\n",
    "  - **spark.databricks.delta.autoCompact.enabled = true**\n",
    "    - Ativa a compactação automática de arquivos pequenos após escrita, evitando muitos pequenos arquivos.\n",
    "    - Pequenos arquivos impactam a performance de leitura; isso mantém o Delta otimizado.\n",
    "\n",
    "  - **spark.sql.autoBroadcastJoinThreshold = 104857600 (100 MB)**\n",
    "    - Define o limite máximo para broadcast join automático.\n",
    "    - Broadcast join é eficiente para joins onde uma tabela é pequena o suficiente para ser replicada em cada executor.\n",
    "    - Um valor maior (100MB) permite mais casos de broadcast join, acelerando a execução de joins na camada Gold.\n",
    "\n",
    "  - **spark.sql.parquet.filterPushdown = true**\n",
    "    - Permite que filtros sejam aplicados diretamente na leitura de arquivos Parquet (e Delta).\n",
    "    - Reduz a quantidade de dados carregados em memória, acelerando consultas.\n",
    "\n",
    "Por que aplicar essas configs só na camada Gold?\n",
    "  - Camada Gold é a mais refinada e consumida por BI/analytics, então a prioridade é performance e otimização para consultas rápidas.\n",
    "  - As operações geralmente envolvem joins complexos, agregações e leituras frequentes.\n",
    "  - Essas configs ajudam o Spark a otimizar shuffle, paralelismo, joins e escrita, melhorando latência e throughput.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb529866-c162-4302-9499-e914bc99ff66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 15:50:09,214 - INFO - Iniciando processamento da camada Gold\n2025-06-15 15:50:09,214 - INFO - Lendo dados da Silver: orders_cleaned\n2025-06-15 15:50:09,215 - INFO - Lendo dados da Silver: order_items_cleaned\n2025-06-15 15:50:09,216 - INFO - Lendo dados da Silver: inventory_cleaned\n2025-06-15 15:50:09,217 - INFO - Construindo tabela de fato: fact_sales\n2025-06-15 15:50:09,217 - INFO - Escrevendo fact_sales na Gold com particionamento por ingest_date\n2025-06-15 15:50:15,677 - INFO - Lendo dados da Silver: customers_cleaned\n2025-06-15 15:50:15,678 - INFO - Construindo dimensão: dim_customers\n2025-06-15 15:50:15,679 - INFO - Escrevendo dim_customers na Gold com particionamento por ingest_date\n2025-06-15 15:50:19,954 - INFO - Lendo dados da Silver: products_cleaned\n2025-06-15 15:50:19,955 - INFO - Construindo dimensão: dim_products\n2025-06-15 15:50:19,959 - INFO - Escrevendo dim_products na Gold com particionamento por ingest_date\n2025-06-15 15:50:23,284 - INFO - Lendo dados da Silver: inventory_cleaned\n2025-06-15 15:50:23,284 - INFO - Construindo tabela agregada: current_inventory\n2025-06-15 15:50:23,641 - INFO - Escrevendo current_inventory na Gold com particionamento por ingest_date\n2025-06-15 15:50:26,529 - INFO - Camada Gold finalizada com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Define os caminhos base das camadas silver e Gold\n",
    "silver_schema = \"lakehouse.a_silver\"\n",
    "gold_schema = \"lakehouse.a_gold\"\n",
    "\n",
    "# Define a data de ingestão a ser processada (ex: '2025-06-12')\n",
    "ingest_date = \"2025-06-15\"\n",
    "\n",
    "# Chama a função principal da camada Silver\n",
    "main(spark, silver_schema, gold_schema, ingest_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fde0e28-4ced-41b9-b6ec-1d5efbeaa4c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
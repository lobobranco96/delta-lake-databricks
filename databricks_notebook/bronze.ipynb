{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca37468-8662-4306-9ee8-416994aaa5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from settings import create_spark_session\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d889d830-ea00-4540-9c8e-1b6301e54dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_with_validation(spark, path, ingest_date):\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .option(\"badRecordsPath\", f\"/tmp/bad_records/{ingest_date}\")\n",
    "            .csv(path)\n",
    "        )\n",
    "        df = df.withColumn(\"ingest_date\", lit(ingest_date))\n",
    "        logger.info(f\"Leitura concluída para {path} com {df.count()} registros.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao ler {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_table(df, table_name, bronze_schema, partition_col):\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(partition_col) \\\n",
    "        .saveAsTable(f\"{bronze_schema}.{table_name}\")\n",
    "\n",
    "def main(spark, raw_path, bronze_schema, ingest_date):\n",
    "\n",
    "    logger.info(\"Iniciando ingestão na camada Bronze...\")\n",
    "\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema}\") # criação do schema se não existir\n",
    "\n",
    "    tables = [\"orders\", \"customers\", \"products\", \"order_items\", \"inventory_updates\"]\n",
    "\n",
    "    for table in tables:\n",
    "        file_path = f\"{raw_path}/{table}.csv\"\n",
    "        logger.info(f\"Lendo a tabela {table} de {file_path}\")\n",
    "        df = read_csv_with_validation(spark, file_path, ingest_date)\n",
    "\n",
    "        if df:\n",
    "            table_name = table\n",
    "            try:\n",
    "                save_table(df, table_name, bronze_schema, \"ingest_date\")\n",
    "                logger.info(f\"Dados gravados com sucesso em {table_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erro ao gravar Delta para {table}: {e}\")\n",
    "        else:\n",
    "            logger.warning(f\"Tabela {table} não foi processada.\")\n",
    "\n",
    "    logger.info(\"Finalizando sessão spark.\")\n",
    "    spark.stop()\n",
    "    \n",
    "    logger.info(\"Ingestão Bronze finalizada com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5761399-bd2e-450e-a889-d83354d52f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 13:45:12,648 - INFO - Iniciando ingestão na camada Bronze...\n2025-06-15 13:45:12,650 - INFO - Lendo a tabela orders de /Volumes/workspace/lakehouse/raw/2025-06-15/orders.csv\n2025-06-15 13:45:13,870 - INFO - Leitura concluída para /Volumes/workspace/lakehouse/raw/2025-06-15/orders.csv com 300000 registros.\n2025-06-15 13:45:17,687 - INFO - Dados gravados com sucesso em orders\n2025-06-15 13:45:17,688 - INFO - Lendo a tabela customers de /Volumes/workspace/lakehouse/raw/2025-06-15/customers.csv\n2025-06-15 13:45:18,808 - INFO - Leitura concluída para /Volumes/workspace/lakehouse/raw/2025-06-15/customers.csv com 196130 registros.\n2025-06-15 13:45:22,340 - INFO - Dados gravados com sucesso em customers\n2025-06-15 13:45:22,341 - INFO - Lendo a tabela products de /Volumes/workspace/lakehouse/raw/2025-06-15/products.csv\n2025-06-15 13:45:23,191 - INFO - Leitura concluída para /Volumes/workspace/lakehouse/raw/2025-06-15/products.csv com 4904 registros.\n2025-06-15 13:45:25,785 - INFO - Dados gravados com sucesso em products\n2025-06-15 13:45:25,786 - INFO - Lendo a tabela order_items de /Volumes/workspace/lakehouse/raw/2025-06-15/order_items.csv\n2025-06-15 13:45:27,203 - INFO - Leitura concluída para /Volumes/workspace/lakehouse/raw/2025-06-15/order_items.csv com 900624 registros.\n2025-06-15 13:45:31,074 - INFO - Dados gravados com sucesso em order_items\n2025-06-15 13:45:31,075 - INFO - Lendo a tabela inventory_updates de /Volumes/workspace/lakehouse/raw/2025-06-15/inventory_updates.csv\n2025-06-15 13:45:31,861 - INFO - Leitura concluída para /Volumes/workspace/lakehouse/raw/2025-06-15/inventory_updates.csv com 9931 registros.\n2025-06-15 13:45:34,526 - INFO - Dados gravados com sucesso em inventory_updates\n2025-06-15 13:45:34,526 - INFO - Ingestão Bronze finalizada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuração do Logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"bronze_layer\")\n",
    "\n",
    "bronze_config = {\n",
    "    \"spark.sql.shuffle.partitions\": \"100\",\n",
    "    \"spark.databricks.delta.optimizeWrite.enabled\": \"true\",\n",
    "    \"spark.databricks.delta.autoCompact.enabled\": \"true\",\n",
    "    \"spark.sql.parquet.filterPushdown\": \"true\",\n",
    "    \"spark.sql.parquet.mergeSchema\": \"false\",\n",
    "    \"spark.sql.files.ignoreCorruptFiles\": \"true\",\n",
    "    \"spark.sql.files.ignoreMissingFiles\": \"true\",\n",
    "}\n",
    "\n",
    "app_name = \"BronzeLayer\"\n",
    "spark = create_spark_session(app_name, bronze_config)\n",
    "\n",
    "# Variaveis\n",
    "ingest_date = \"2025-06-15\"\n",
    "raw_path = f\"/Volumes/workspace/lakehouse/a_raw/{ingest_date}\"\n",
    "bronze_schema = \"lakehouse.a_bronze\"\n",
    "\n",
    "main(spark, raw_path, bronze_schema, ingest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2719e0bd-b4be-47d8-a16c-1572457f6bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Explicação das configs para a camada bronze\n",
    "  - **spark.sql.shuffle.partitions = 100**\n",
    "    - Ingestões da Bronze geralmente envolvem menos transformações, mas ainda podem haver repartições.\n",
    "    - Um número menor (comparado à Silver e Gold) ajuda a evitar overhead desnecessário.\n",
    "\n",
    "  - **spark.databricks.delta.optimizeWrite.enabled = true**\n",
    "    - Mantém a escrita mais eficiente mesmo com dados brutos.\n",
    "    - Reduz a criação de muitos arquivos pequenos.\n",
    "\n",
    "  - **spark.databricks.delta.autoCompact.enabled = true**\n",
    "    - Ajuda a consolidar arquivos pequenos automaticamente.\n",
    "    - Importante para evitar \"file explosion\" após ingestões frequentes ou em micro-batches.\n",
    "\n",
    "  - **spark.sql.parquet.filterPushdown = true**\n",
    "    - Já ativa desde a Bronze para aproveitar filtros mesmo nos dados brutos quando possível.\n",
    "    - Economiza I/O mesmo antes das transformações.\n",
    "\n",
    "  - **spark.sql.parquet.mergeSchema = false**\n",
    "    - Evita sobrecarga desnecessária tentando unificar esquemas automaticamente.\n",
    "    - Na Bronze, o schema deve ser fixo ou controlado, não tolerando variações inesperadas.\n",
    "\n",
    "  - **spark.sql.files.ignoreCorruptFiles = true**\n",
    "    - Permite que arquivos corrompidos não interrompam o processo de ingestão.\n",
    "    - Crítico para garantir robustez em ambientes de ingestão contínua.\n",
    "\n",
    "  - **spark.sql.files.ignoreMissingFiles = true**\n",
    "    - Similar ao anterior: evita falhas se algum arquivo referenciado tiver sido excluído ou estiver faltando.\n",
    "\n",
    "Essa configuração prioriza:\n",
    "  - Tolerância a falhas → evita que arquivos ruins interrompam o pipeline\n",
    "  - Escrita eficiente → com Delta otimizado e compactação\n",
    "  - Leitura segura → com pushdown e controle rígido de schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3f7406-18f0-4f7a-be93-d489e1bdf3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
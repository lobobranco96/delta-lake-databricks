{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f04a6ae-bc2e-437a-94e4-33f30088b33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from settings import create_spark_session\n",
    "from pyspark.sql.functions import col, to_date, lit, trim\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb35644c-300c-42c8-8693-f78e982259d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_table(df, table_name, silver_schema, partition_col):\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(partition_col) \\\n",
    "        .saveAsTable(f\"{silver_schema}.{table_name}\")\n",
    "\n",
    "def process_customers(logger, spark, bronze_schema, silver_schema, ingest_date):\n",
    "    try:\n",
    "        logger.info(\"Lendo dados da camada Bronze - customers...\")\n",
    "\n",
    "        customers = spark.table(f\"{bronze_schema}.customers\") \\\n",
    "            .filter(col(\"ingest_date\") == ingest_date)\n",
    "\n",
    "        if customers.count() == 0:\n",
    "            logger.warning(f\"Nenhum dado encontrado para ingest_date = {ingest_date}.\")\n",
    "            return\n",
    "        else:\n",
    "            logger.info(\"Transformando e limpando dados dos clientes...\")\n",
    "            customers_cleaned = (\n",
    "                customers\n",
    "                .dropDuplicates([\"customer_id\"])\n",
    "                .filter(col(\"email\").isNotNull())\n",
    "                .filter(col(\"created_at\") <= to_date(lit(ingest_date), \"yyyy-MM-dd\"))\n",
    "                .withColumn(\"address\", trim(col(\"address\")))\n",
    "            )\n",
    "\n",
    "            logger.info(\"Salvando clientes limpos na camada Silver como tabela...\")\n",
    "            save_table(customers_cleaned, \"customers_cleaned\", silver_schema, \"ingest_date\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao processar customers: {e}\")\n",
    "\n",
    "def process_products(logger, spark, bronze_schema, silver_schema, ingest_date):\n",
    "    try:\n",
    "        logger.info(\"Lendo dados da camada Bronze - products...\")\n",
    "\n",
    "        products = spark.table(f\"{bronze_schema}.products\") \\\n",
    "            .filter(col(\"ingest_date\") == ingest_date)\n",
    "        \n",
    "        if products.count() == 0:\n",
    "            logger.warning(f\"Nenhum dado encontrado para ingest_date = {ingest_date}.\")\n",
    "        else:\n",
    "            logger.info(\"Transformando e limpando dados dos produtos...\")\n",
    "            products_cleaned = (\n",
    "                products\n",
    "                .dropDuplicates([\"product_id\"])\n",
    "                .filter(col(\"price\") > 0)\n",
    "                .filter(col(\"category\") != \"\")\n",
    "            )\n",
    "\n",
    "            logger.info(\"Salvando produtos limpos na camada Silver como tabela...\")\n",
    "            save_table(products_cleaned, \"products_cleaned\", silver_schema, \"ingest_date\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao processar products: {e}\")\n",
    "\n",
    "def process_inventory_updates(logger, spark, bronze_schema, silver_schema, ingest_date):\n",
    "    try:\n",
    "        logger.info(\"Lendo dados da camada Bronze - inventory_updates...\")\n",
    "\n",
    "        inventory = spark.table(f\"{bronze_schema}.inventory_updates\") \\\n",
    "            .filter(col(\"ingest_date\") == ingest_date)\n",
    "        \n",
    "        if inventory.count() == 0:\n",
    "            logger.warning(f\"Nenhum dado encontrado para ingest_date = {ingest_date}.\")\n",
    "        else:\n",
    "            logger.info(\"Transformando e limpando dados de inventário...\")\n",
    "            inventory_cleaned = (\n",
    "                inventory\n",
    "                .filter(col(\"change\") != 0)\n",
    "                .filter(col(\"timestamp\") <= lit(datetime.today().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "            )\n",
    "\n",
    "            logger.info(\"Salvando inventário limpo na camada Silver como tabela...\")\n",
    "            save_table(inventory_cleaned, \"inventory_cleaned\", silver_schema, \"ingest_date\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao processar inventory_updates: {e}\")\n",
    "\n",
    "def process_orders(logger, spark, bronze_schema, silver_schema, ingest_date):\n",
    "    try:\n",
    "        logger.info(\"Lendo dados da camada Bronze - orders...\")\n",
    "\n",
    "        orders = spark.table(f\"{bronze_schema}.orders\") \\\n",
    "            .filter(col(\"ingest_date\") == ingest_date)\n",
    "        \n",
    "        if orders.count() == 0:\n",
    "            logger.warning(f\"Nenhum dado encontrado para ingest_date = {ingest_date}.\")\n",
    "        else:\n",
    "            logger.info(\"Transformando e limpando dados dos pedidos...\")\n",
    "\n",
    "            orders_cleaned = (\n",
    "                orders\n",
    "                .withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                .filter(col(\"status\").isin([\"completed\", \"processing\"]))\n",
    "                .dropDuplicates([\"order_id\"])\n",
    "            )\n",
    "\n",
    "            logger.info(\"Salvando pedidos limpos na camada Silver como tabela...\")\n",
    "            save_table(orders_cleaned, \"orders_cleaned\", silver_schema, \"ingest_date\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao processar orders: {e}\")\n",
    "\n",
    "def process_order_items(logger, spark, bronze_schema, silver_schema, ingest_date):\n",
    "    try:\n",
    "        logger.info(\"Lendo dados da camada Bronze - order_items...\")\n",
    "\n",
    "        items = spark.table(f\"{bronze_schema}.order_items\") \\\n",
    "            .filter(col(\"ingest_date\") == ingest_date)\n",
    "        \n",
    "        if items.count() == 0:\n",
    "            logger.warning(f\"Nenhum dado encontrado para ingest_date = {ingest_date}.\")\n",
    "        else:\n",
    "            logger.info(\"Transformando e limpando dados dos itens dos pedidos...\")\n",
    "            items_cleaned = (\n",
    "                items\n",
    "                .filter(col(\"quantity\") > 0)\n",
    "                .filter(col(\"unit_price\") > 0)\n",
    "                .withColumn(\"total_price\", col(\"quantity\") * col(\"unit_price\"))\n",
    "            )\n",
    "\n",
    "            logger.info(\"Salvando itens de pedido limpos na camada Silver como tabela...\")\n",
    "            save_table(items_cleaned, \"order_items_cleaned\", silver_schema, \"ingest_date\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao processar order_items: {e}\")\n",
    "\n",
    "def main(spark, bronze_schema, silver_schema, ingest_date):\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger(\"SilverLayer\")\n",
    "\n",
    "    # Cria o schema Silver\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {silver_schema}\")\n",
    "\n",
    "    logger.info(\"Iniciando o processamento da camada Silver.\")\n",
    "\n",
    "    process_customers(logger, spark, bronze_schema, silver_schema, ingest_date)\n",
    "    process_inventory_updates(logger, spark, bronze_schema, silver_schema, ingest_date)\n",
    "    process_order_items(logger, spark, bronze_schema, silver_schema, ingest_date)\n",
    "    process_orders(logger, spark, bronze_schema, silver_schema, ingest_date)\n",
    "    process_products(logger, spark, bronze_schema, silver_schema, ingest_date)\n",
    "\n",
    "    logger.info(\"Finalizando sessão spark.\")\n",
    "    spark.stop()\n",
    "    logger.info(\"Processamento da camada Silver finalizado com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63acad8d-81ad-4dbe-bd05-9126d01e51ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_config = {\n",
    "    \"spark.sql.shuffle.partitions\": \"200\",\n",
    "    \"spark.databricks.delta.optimizeWrite.enabled\": \"true\",\n",
    "    \"spark.databricks.delta.autoCompact.enabled\": \"true\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\": \"104857600\",\n",
    "    \"spark.default.parallelism\": \"200\",\n",
    "    \"spark.sql.parquet.filterPushdown\": \"true\",\n",
    "    \"spark.databricks.delta.merge.enabled\": \"true\",\n",
    "}\n",
    "\n",
    "app_name = \"SilverLayer\"\n",
    "spark = create_spark_session(app_name, silver_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a2160a4-b9da-4079-8e57-64ef9c5d7668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Explicação das configs para a camada silver\n",
    "  - **spark.sql.shuffle.partitions = 200**\n",
    "    - Aumenta a paralelização de operações como groupBy, join, repartition.\n",
    "    - Ideal para datasets maiores, onde ocorrem limpezas e agregações.\n",
    "\n",
    "  - **spark.databricks.delta.optimizeWrite.enabled = true**\n",
    "    - Durante as transformações e escrita intermediária, garante menos arquivos pequenos.\n",
    "    - Evita overhead futuro na leitura.\n",
    "\n",
    "  - **spark.databricks.delta.autoCompact.enabled = true**\n",
    "    - Compacta arquivos pequenos automaticamente após escrita.\n",
    "    - Mantém a Silver eficiente para leitura na Gold.\n",
    "  \n",
    "  - **spark.sql.autoBroadcastJoinThreshold = 104857600**\n",
    "    - Permite que joins sejam feitos por broadcast quando a tabela for menor que 100 MB.\n",
    "    - Evita shuffles desnecessários e acelera joins.\n",
    "  \n",
    "  - **spark.default.parallelism = 200**\n",
    "    - Ajusta o nível de paralelismo padrão para tarefas como leitura/escrita paralela, sem shuffle.\n",
    "    - Coerente com o número de partitions.\n",
    "\n",
    "  - **spark.sql.parquet.filterPushdown = true**\n",
    "    - Usa filtros na leitura de arquivos Parquet, economizando leitura de dados não relevantes.\n",
    "    - Já útil na Silver ao reprocessar ou validar dados.\n",
    "\n",
    "  - **spark.databricks.delta.merge.enabled = true**\n",
    "    - Habilita suporte interno para comandos MERGE INTO, comuns na Silver.\n",
    "    - Útil caso aplique upserts (ex: merge de batches ou dados CDC).\n",
    "\n",
    "Essa configuração é ideal para a camada Silver, pois:\n",
    "  - Aumenta a eficiência das transformações.\n",
    "  - Mantém a escrita estruturada e otimizada.\n",
    "  - Suporta operações como deduplicação, joins e merge com performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09844d31-e664-4d70-ad4a-e929d831b123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 13:47:17,313 - INFO - Iniciando o processamento da camada Silver.\n2025-06-15 13:47:17,315 - INFO - Lendo dados da camada Bronze - customers...\n2025-06-15 13:47:17,750 - INFO - Transformando e limpando dados dos clientes...\n2025-06-15 13:47:17,751 - INFO - Salvando clientes limpos na camada Silver como tabela...\n2025-06-15 13:47:21,523 - INFO - Lendo dados da camada Bronze - inventory_updates...\n2025-06-15 13:47:21,838 - INFO - Transformando e limpando dados de inventário...\n2025-06-15 13:47:21,841 - INFO - Salvando inventário limpo na camada Silver como tabela...\n2025-06-15 13:47:24,240 - INFO - Lendo dados da camada Bronze - order_items...\n2025-06-15 13:47:24,544 - INFO - Transformando e limpando dados dos itens dos pedidos...\n2025-06-15 13:47:24,545 - INFO - Salvando itens de pedido limpos na camada Silver como tabela...\n2025-06-15 13:47:29,198 - INFO - Lendo dados da camada Bronze - orders...\n2025-06-15 13:47:29,533 - INFO - Transformando e limpando dados dos pedidos...\n2025-06-15 13:47:29,534 - INFO - Salvando pedidos limpos na camada Silver como tabela...\n2025-06-15 13:47:32,811 - INFO - Lendo dados da camada Bronze - products...\n2025-06-15 13:47:33,095 - INFO - Transformando e limpando dados dos produtos...\n2025-06-15 13:47:33,096 - INFO - Salvando produtos limpos na camada Silver como tabela...\n2025-06-15 13:47:35,624 - INFO - Processamento da camada Silver finalizado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "#VARIAVEIS\n",
    "bronze_schema = \"lakehouse.a_bronze\"\n",
    "silver_schema = \"lakehouse.a_silver\"\n",
    "ingest_date = \"2025-06-15\" # Define a data de ingestão a ser processada (ex: '2025-06-12')\n",
    "\n",
    "# Chama a função principal da camada Silver\n",
    "main(spark, bronze_schema, silver_schema, ingest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb67c95-4503-4312-9bf8-99ebf8be2b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
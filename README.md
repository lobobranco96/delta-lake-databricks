# Projeto Delta Lakehouse com Databricks

Este projeto implementa uma arquitetura moderna de dados (Lakehouse) no Databricks na free edition, utilizando Delta Lake e Pyspark com as camadas **Bronze**, **Silver** e **Gold**. A orquestraÃ§Ã£o Ã© realizada por meio de notebooks organizados e versionados via GitHub.

---

## Estrutura das Camadas

- **ðŸ”¹ Bronze**: IngestÃ£o de dados brutos a partir de arquivos CSV.
- **âšª Silver**: Limpeza, deduplicaÃ§Ã£o e padronizaÃ§Ã£o de dados.
- **ðŸŸ¡ Gold**: AgregaÃ§Ãµes e modelo analÃ­tico final (curated).

---

## Tecnologias Utilizadas

- [Databricks](https://databricks.com/) - Versao gratuida
- Delta Lake (formato de armazenamento)
- Apache Spark (Spark SQL e Pyspark)
- GitHub (versionamento de notebooks)
- Markdown (documentaÃ§Ã£o)

---

## ðŸ“‚ Estrutura de Pastas
```lua
â”œâ”€â”€ databricks_notebook
â”‚   â”œâ”€â”€ bronze.ipynb
â”‚   â”œâ”€â”€ gold.ipynb
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ silver.ipynb
â”œâ”€â”€ desenvolvimento_git.ipynb
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ catalog_lakehouse.JPG
â”‚   â”œâ”€â”€ job_complete1.JPG
â”‚   â”œâ”€â”€ job_complete2.JPG
â”‚   â””â”€â”€ job_complete3.JPG
â”œâ”€â”€ raw_data
â”‚   â”œâ”€â”€ customers.csv
â”‚   â”œâ”€â”€ inventory_updates.csv
â”‚   â”œâ”€â”€ order_items.csv
â”‚   â”œâ”€â”€ orders.csv
â”‚   â””â”€â”€ products.csv
â””â”€â”€ README.MD
```

---

## ExecuÃ§Ã£o com Databricks Jobs Pipelines
  - Este projeto utiliza os Jobs Pipelines do Databricks para orquestrar a execuÃ§Ã£o dos notebooks que compÃµem cada camada da arquitetura Lakehouse (Bronze, Silver e Gold). Os notebooks sÃ£o versionados em um repositÃ³rio GitHub e conectados diretamente ao workspace do Databricks via a funcionalidade de Repos.
  - Cada notebook representa uma etapa do pipeline:
    - **bronze**.ipynb: ResponsÃ¡vel pela ingestÃ£o dos dados brutos.
    - **silver**.ipynb: Realiza limpeza, filtragem e deduplicaÃ§Ã£o dos dados.
    - **gold**.ipynb: Aplica agregaÃ§Ãµes, enriquecimentos e transforma os dados para consumo analÃ­tico.

  - Os notebooks sÃ£o executados em sequÃªncia por um Job configurado na interface do Databricks, garantindo automaÃ§Ã£o e rastreabilidade dos processamentos.

## ConfiguraÃ§Ãµes do Job
  - Tipo: Job Pipeline
  - Fonte: GitHub Repo conectado
  - Ambiente: Cluster compartilhado com Delta Lake ativado
  - Ordem de execuÃ§Ã£o: Bronze â†’ Silver â†’ Gold

---

## OrganizaÃ§Ã£o do Lakehouse e CatÃ¡logo**
Este projeto segue o padrÃ£o Lakehouse Architecture utilizando Delta Lake no Databricks, com dados organizados em trÃªs camadas: Raw, Bronze, Silver e Gold.

## CatÃ¡logo Personalizado
  - Todos os dados e tabelas Delta sÃ£o salvos dentro de um catÃ¡logo personalizado chamado:
  - catalog: lakehouse
  - Dentro do catÃ¡logo lakehouse, os dados sÃ£o organizados por esquemas (schemas) que representam as camadas:
    - raw â€“ Armazena os arquivos brutos organizados por data de ingestÃ£o.
    - bronze â€“ Tabelas Delta com dados normalizados e padronizados.
    - silver â€“ Tabelas Delta limpas e transformadas.
    - gold â€“ Tabelas Delta analÃ­ticas e agregadas.

  - AlÃ©m de utilizar o armazenamento interno do Workspace do Databricks, uma boa opÃ§Ã£o tambÃ©m, o databricks permite configurar um Cloud Storage externo como fonte de dados para a camada Raw, como:
    - Amazon S3
    - Google Cloud Storage (GCS)
    - Azure Data Lake Storage (ADLS)
    Que por sinal tem bastante beneficios como:
      - Evita replicaÃ§Ã£o desnecessÃ¡ria de dados.
      - MantÃ©m os dados centralizados em uma fonte de verdade.
      - Permite que o Lakehouse seja consumido a partir de qualquer cloud provider, nÃ£o limitado ao Databricks Workspace.


## ConfiguraÃ§Ãµes do Spark por Camada

Cada camada da arquitetura (Bronze, Silver e Gold) utiliza uma SparkSession com configuraÃ§Ãµes personalizadas para atender Ã s necessidades de performance e otimizaÃ§Ã£o em cada etapa do pipeline:

**Bronze** IngestÃ£o dos Dados Brutos
```bash
bronze_config = {
    "spark.sql.shuffle.partitions": "100",
    "spark.databricks.delta.optimizeWrite.enabled": "true",
    "spark.databricks.delta.autoCompact.enabled": "true",
    "spark.sql.parquet.filterPushdown": "true",
    "spark.sql.parquet.mergeSchema": "false",
    "spark.sql.files.ignoreCorruptFiles": "true",
    "spark.sql.files.ignoreMissingFiles": "true",
}
```
**Objetivo**: Evitar falhas na leitura, ignorar arquivos corrompidos e otimizar escrita e compactaÃ§Ã£o automÃ¡tica.

**Silver** Limpeza e PadronizaÃ§Ã£o dos Dados
```bash
silver_config = {
    "spark.sql.shuffle.partitions": "200",
    "spark.databricks.delta.optimizeWrite.enabled": "true",
    "spark.databricks.delta.autoCompact.enabled": "true",
    "spark.sql.autoBroadcastJoinThreshold": "104857600",
    "spark.default.parallelism": "200",
    "spark.sql.parquet.filterPushdown": "true",
    "spark.databricks.delta.merge.enabled": "true",
}
```
**Objetivo**: Melhorar o desempenho de joins e atualizaÃ§Ãµes com Delta.
**BenefÃ­cios**: OtimizaÃ§Ã£o de escrita, joins eficientes e uso de broadcast quando viÃ¡vel.

**Gold** (Camada AnalÃ­tica)

```bash
gold_config = {
    "spark.sql.shuffle.partitions": "300",
    "spark.default.parallelism": "300",
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.skewJoin.enabled": "true",
    "spark.databricks.delta.optimizeWrite.enabled": "true",
    "spark.databricks.delta.autoCompact.enabled": "true",
    "spark.sql.autoBroadcastJoinThreshold": "104857600",
    "spark.sql.parquet.filterPushdown": "true"
}
```
**Objetivo**: Ganho de performance com Adaptive Query Execution (AQE) em joins pesados.
**BenefÃ­cios**: Performance aprimorada em workloads analÃ­ticos com dados skewed (desequilibrados).

Essas configuraÃ§Ãµes sÃ£o aplicadas dinamicamente em cada notebook ao iniciar a SparkSession, garantindo que cada etapa esteja otimizada para sua funÃ§Ã£o dentro da arquitetura Lakehouse.

